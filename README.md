# ğŸ› ï¸ Retail Order ETL Project

Welcome to the **Retail Order ETL Project** repository! ğŸš€
This project demonstrates a complete ETL (Extract, Transform, Load) pipeline to process and load retail order data from multiple CSV files into a PostgreSQL relational database using Python.

---
## ğŸ“– Project Overview
This project involves:
1. **Data Architecture**: Lays the foundation for scalable data warehousing using principles aligned with Medallion Architecture â€” organizing data into Bronze (raw), Silver (cleaned), and Gold (aggregated/reporting) layers.
2. **ETL Pinelines**: Using Python scripts to extract data from source CSVs, transform and clean it, and load it into PostgreSQL.
3. **Data Modeling**: Implements a relational schema with fact and dimension tables optimized for analytics, ensuring data integrity with foreign keys and normalization.
4. **Analytics & Reporting**: Enables SQL-based insights on sales, customers, and operations with queries and future dashboard plans.

---

## ğŸš€ Project Requirements

### Building a retail ETL data pipeline (Data Engineering)

---
## ğŸ“¦ Technology Used
- **Python** (pandas, psycopg2, dotenv)
- **PostgreSQL** (local database)
- **pgAdmin** for GUI access
- **draw.io** for data modeling
- **VS Code** for development

---
## ğŸ—‚ï¸ Folder Structure
```text
retail-etl-pipeline/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ rawData/                 # Input data (CSV/JSON)
â”‚   â””â”€â”€ processedData/           # Cleaned/transformed data
â”œâ”€â”€ diagrams/
â”‚   â””â”€â”€ data_model.drawio        # ER diagram for database tables
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py               # Read data from raw files
â”‚   â”œâ”€â”€ transform.py             # Clean, join, enrich data
â”‚   â””â”€â”€ load.py                  # Load data into PostgreSQL
â””â”€â”€ scripts/
    â””â”€â”€ main.py                  # Orchestrates the full ETL
â”œâ”€â”€ README.md
â”œâ”€â”€ .env                         # DB credentials (never commit to GitHub)
â”œâ”€â”€ requirements.txt
```
---
##  âš™ï¸ ETL Workflow (Retail Scenario)
### 1. Extract 
- Load 5 raw CSV files: customers, orders, orderitems, products, payments
- Preview and analyze the structure

### 2. Transform 
- Normalize column formats (upper/lower case, strip spaces)
- Convert timestamps to datetime
- Drop nulls and duplicates
- Save cleaned versions to data/clean_data/

### 3. Load
- Create PostgreSQL tables with foreign key constraints
- Load each table in the correct dependency order
- Skip invalid FK rows and log skipped rows

### 4. Data Validation
- Check row counts per table
- Detect null values in each column
- Compare database vs CSV rows
  
---
### ğŸš€ How to Run
1. Set up and activate Python virtual environment:
  - MacOS/Linux: python3 -m venv env, then source env/bin/activate
  - Windows: python virtualenv env, then ./env/scripts/activate
2. Install required packages: pip install -r requirements.txt
3. Set up PostgreSQL
  - Create a database: retail_order_db
  - Create tables using pgAdmin or schema.sql
  - Add .env file and set the environment variables
4. Run pipeline

---
##  Author
**Ngan Huynh**

Data Engineering Student & ETL Builder

---
## ğŸ“ License
MIT License

